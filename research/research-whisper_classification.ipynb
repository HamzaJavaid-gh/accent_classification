{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905a3a0a",
   "metadata": {},
   "source": [
    "### Accent Classification with OpenAI Whisper - Research POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f22c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06065ba0",
   "metadata": {},
   "source": [
    "### Setting up FFMPEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a709f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "\n",
    "# Confirm ffmpeg is now discoverable\n",
    "import shutil\n",
    "print(\"FFmpeg path:\", shutil.which(\"ffmpeg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f401b",
   "metadata": {},
   "source": [
    "### Setting up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = '/Users/zp3146/Desktop/projects_hamza/accent_classification/research/english_us.mp3'\n",
    "whsiper_model = 'turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f436d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(whsiper_model)\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(audio_file_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a685e4",
   "metadata": {},
   "source": [
    "### End - End Accent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import whisper\n",
    "from whisper.audio import N_FRAMES, N_MELS, log_mel_spectrogram, pad_or_trim\n",
    "from whisper.model import Whisper\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these to your paths\n",
    "audio_file_path = \"/Users/zp3146/Desktop/projects_hamza/accent_classification/research/english_uk.wav\"\n",
    "class_names_file_path = \"/Users/zp3146/Desktop/projects_hamza/accent_classification/research/class_names.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69708f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AudioData:\n",
    "    audio_path: str\n",
    "    category: Optional[str] = None\n",
    "\n",
    "\n",
    "def read_class_names(path: str) -> List[str]:\n",
    "    with open(path) as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_audio_features(audio_path: Optional[str], model: Whisper) -> torch.Tensor:\n",
    "    if audio_path is None:\n",
    "        segment = torch.zeros((N_MELS, N_FRAMES), dtype=torch.float32).to(model.device)\n",
    "    else:\n",
    "        mel = log_mel_spectrogram(audio_path)\n",
    "        segment = pad_or_trim(mel, N_FRAMES).to(model.device)\n",
    "    return model.embed_audio(segment.unsqueeze(0))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_average_logprobs(\n",
    "    model: Whisper,\n",
    "    audio_features: torch.Tensor,\n",
    "    class_names: List[str],\n",
    "    tokenizer,\n",
    ") -> torch.Tensor:\n",
    "    initial_tokens = (\n",
    "        torch.tensor(tokenizer.sot_sequence_including_notimestamps).unsqueeze(0).to(model.device)\n",
    "    )\n",
    "    eot_token = torch.tensor([tokenizer.eot]).unsqueeze(0).to(model.device)\n",
    "\n",
    "    average_logprobs = torch.zeros(len(class_names))\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_name_tokens = (\n",
    "            torch.tensor(tokenizer.encode(\" \" + class_name)).unsqueeze(0).to(model.device)\n",
    "        )\n",
    "        input_tokens = torch.cat([initial_tokens, class_name_tokens, eot_token], dim=1)\n",
    "\n",
    "        logits = model.logits(input_tokens, audio_features)  # (1, T, V)\n",
    "        logprobs = F.log_softmax(logits, dim=-1).squeeze(0)  # (T, V)\n",
    "        logprobs = logprobs[len(tokenizer.sot_sequence_including_notimestamps) - 1 : -1]  # (T', V)\n",
    "        logprobs = torch.gather(logprobs, dim=-1, index=class_name_tokens.view(-1, 1))  # (T', 1)\n",
    "        average_logprob = logprobs.mean().item()\n",
    "        average_logprobs[i] = average_logprob\n",
    "\n",
    "    return average_logprobs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def classify(\n",
    "    model: Whisper,\n",
    "    audio_path: str,\n",
    "    class_names: List[str],\n",
    "    tokenizer,\n",
    "    internal_lm_average_logprobs: Optional[torch.Tensor] = None,\n",
    "    verbose: bool = True,\n",
    ") -> str:\n",
    "    audio_features = calculate_audio_features(audio_path, model)\n",
    "\n",
    "    average_logprobs = calculate_average_logprobs(\n",
    "        model=model,\n",
    "        audio_features=audio_features,\n",
    "        class_names=class_names,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    if internal_lm_average_logprobs is not None:\n",
    "        average_logprobs -= internal_lm_average_logprobs\n",
    "\n",
    "    sorted_indices = sorted(\n",
    "        range(len(class_names)), key=lambda i: average_logprobs[i], reverse=True\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"üîç Log probabilities for each class:\")\n",
    "        for i in sorted_indices:\n",
    "            print(f\"  {class_names[i]}: {average_logprobs[i]:.3f}\")\n",
    "\n",
    "    return class_names[sorted_indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = whisper.load_model(\"large\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = get_tokenizer(multilingual=True, language=\"en\")\n",
    "\n",
    "# Read class names\n",
    "class_names = read_class_names(class_names_file_path)\n",
    "\n",
    "# Classify\n",
    "predicted_class = classify(\n",
    "    model=model,\n",
    "    audio_path=audio_file_path,\n",
    "    class_names=class_names,\n",
    "    tokenizer=tokenizer,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Final predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accent_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
